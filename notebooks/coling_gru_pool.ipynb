{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = str(cpu_count())\n",
    "notebook_dir = !pwd\n",
    "# Some issues with jupyter, so had to add these\n",
    "REPO_PATH = notebook_dir[0].rsplit(\"/notebooks\", 1)[0]\n",
    "sys.path.append(REPO_PATH)\n",
    "# Import Geiger modules\n",
    "from geiger.utils import load_word_vectors\n",
    "from geiger import coling, transform, models, evaluate, stores\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After loading the data, we want to load the data and define some contraints for modelling the sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "x_train, x_dev, y_train, y_dev = coling.load_coling_data(os.path.join(REPO_PATH, \"datasets/coling/english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "n_classes = 3  # {non-aggression, covert aggression, overt aggression}\n",
    "max_features = 30000 # max number of tokens\n",
    "maxlen = 100 # max sequence lenght\n",
    "embed_size = 300 # fastText embedding size\n",
    "batch_size = 32 # batch\n",
    "epochs = 1 # epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just look into the datasets to see if it's all legit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11999, 3)\n",
      "(3001, 3)\n",
      "11999\n",
      "3001\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_dev.shape)\n",
    "print(len(x_train))\n",
    "print(len(x_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment conditions\n",
    "\n",
    "The line above is crucial as it defines the experimental conditions. We have created an abstraction for loading the embeddings, which takes a language parameter and also an `apply_transform` boolean value. With this, we can define two types of embedding lookups:\n",
    "- single language (only english vectors are loaded)\n",
    "- multi language with transform (english and hindi vectors are loaded and the vectors are aligned to a common space using [these transformation matrices](https://github.com/Babylonpartners/fastText_multilingual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from /Users/thiago/code/tgalery/geiger/resources/wiki-news-300d-1M-subword.vec\n",
      "reading word vectors from /Users/thiago/code/tgalery/geiger/resources/wiki-news-300d-1M-subword.vec\n",
      "reading word vectors from /Users/thiago/code/tgalery/geiger/resources/wiki.hi.vec\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding lookup, this takes a while so don't re excute this cell over and over again.\n",
    "single_lang_embed_lookup = stores.MultiLangVectorStore(langs=[\"en\"])\n",
    "multi_lang_embed_lookup = stores.MultiLangVectorStore(langs=[\"en\", \"hi\"], apply_transform=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer and Embedding Matrix\n",
    "For this stage, we create a sequence transformer and embedding matrices for testing the conditions we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the transformer\n",
    "transformer = transform.KerasTransformer(list(x_train) + list(x_dev), max_features, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23800/23800 [00:00<00:00, 63056.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6001 words were out of vocabulary.\n",
      "(23801, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate single lang embedding matrix\n",
    "single_lang_embed_matrix, _ = transformer.generate_embedding_matrix(single_lang_embed_lookup, embed_size)\n",
    "print(single_lang_embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23800/23800 [00:00<00:00, 63019.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6001 words were out of vocabulary.\n",
      "(23801, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate multi lang embedding matrix\n",
    "multi_lang_embed_matrix, _ = transformer.generate_embedding_matrix(multi_lang_embed_lookup, embed_size)\n",
    "print(multi_lang_embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "We can now create the models and evaluate whether there's any improvement in incoporating oov vectors from multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "x_train_pad = transformer.texts_to_seq(x_train)\n",
    "x_dev_pad = transformer.texts_to_seq(x_dev)\n",
    "# Define summary\n",
    "SumEval = evaluate.SummaryEvaluation(validation_data=(x_dev_pad, y_dev), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the single language model\n",
    "single_lang_model = models.build_pooled_gru(n_classes, transformer.rel_features, maxlen, single_lang_embed_matrix, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11999 samples, validate on 3001 samples\n",
      "Epoch 1/1\n",
      "11999/11999 [==============================] - 86s 7ms/step - loss: 0.9593 - acc: 0.5385 - val_loss: 0.8892 - val_acc: 0.5788\n",
      "\n",
      " F1Score - epoch: 1 - score:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.68      0.67      1233\n",
      "          1       0.50      0.57      0.53      1057\n",
      "          2       0.57      0.41      0.48       711\n",
      "\n",
      "avg / total       0.58      0.58      0.58      3001\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the single language model\n",
    "single_lang_hist = single_lang_model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_dev_pad, y_dev),\n",
    "                 callbacks=[SumEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the multi language model\n",
    "multi_lang_model = models.build_pooled_gru(n_classes, transformer.rel_features, maxlen, multi_lang_embed_matrix, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11999 samples, validate on 3001 samples\n",
      "Epoch 1/1\n",
      "11999/11999 [==============================] - 83s 7ms/step - loss: 0.9679 - acc: 0.5328 - val_loss: 0.9387 - val_acc: 0.5545\n",
      "\n",
      " F1Score - epoch: 1 - score:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.78      0.68      1233\n",
      "          1       0.48      0.48      0.48      1057\n",
      "          2       0.61      0.27      0.37       711\n",
      "\n",
      "avg / total       0.56      0.55      0.53      3001\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_lang_hist = multi_lang_model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_dev_pad, y_dev),\n",
    "                 callbacks=[SumEval])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
