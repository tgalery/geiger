{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = str(cpu_count())\n",
    "notebook_dir = !pwd\n",
    "# Some issues with jupyter, so had to add these\n",
    "REPO_PATH = notebook_dir[0].rsplit(\"/notebooks\", 1)[0]\n",
    "sys.path.append(REPO_PATH)\n",
    "# Import Geiger modules\n",
    "from geiger.utils import load_coling_data, load_word_vectors\n",
    "from geiger import coling, transform, models, evaluate, stores\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "x_train, x_dev, y_train, y_dev = load_coling_data(os.path.join(REPO_PATH, \"datasets\"))\n",
    "# Define some constants\n",
    "n_classes = 3\n",
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "batch_size = 16\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11999, 3)\n",
      "(3001, 3)\n",
      "11999\n",
      "3001\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray([coling.one_hot_encode(class_tag) for class_tag in y_train])\n",
    "y_dev = np.asarray([coling.one_hot_encode(class_tag) for class_tag in y_dev])\n",
    "print(y_train.shape)\n",
    "print(y_dev.shape)\n",
    "print(len(x_train))\n",
    "print(len(x_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from /Users/thiago/code/tgalery/geiger/resources/wiki-news-300d-1M-subword.vec\n",
      "reading word vectors from /Users/thiago/code/tgalery/geiger/resources/wiki.hi.vec\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding lookup, this takes a while so don't re excute this cell over and over again.\n",
    "embed_lookup = stores.MultiLangVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22014/22014 [00:00<00:00, 89144.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5949 words were out of vocabulary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the transformer\n",
    "transformer = transform.KerasTransformer(list(x_train) + list(x_dev), max_features, maxlen)\n",
    "# Generate embedding Matrix\n",
    "embed_matrix = transformer.generate_embedding_matrix(embed_lookup, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22015, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = models.build_pooled_gru(n_classes, transformer.rel_features, maxlen, embed_matrix, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = transformer.texts_to_seq(x_train)\n",
    "X_dev = transformer.texts_to_seq(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11999 samples, validate on 3001 samples\n",
      "Epoch 1/1\n",
      "11999/11999 [==============================] - 115s 10ms/step - loss: 0.6536 - acc: 0.7180 - val_loss: 0.9354 - val_acc: 0.5705\n",
      "\n",
      " F1Score - epoch: 1 - score:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.69      0.67      1233\n",
      "          1       0.50      0.54      0.52      1057\n",
      "          2       0.53      0.41      0.46       711\n",
      "\n",
      "avg / total       0.57      0.57      0.57      3001\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RocAuc = evaluate.RocAucEvaluation(validation_data=(X_dev, y_dev), interval=1)\n",
    "SumEval = evaluate.SummaryEvaluation(validation_data=(X_dev, y_dev), interval=1)\n",
    "# Train the model\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_dev),\n",
    "                 callbacks=[SumEval])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
