from keras.models import Model
from keras.layers import Input, Dense, Embedding, GlobalMaxPooling1D, SpatialDropout1D, concatenate
from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, BatchNormalization
from geiger import evaluate


def build_pooled_gru(num_classes, vocab_size, max_seq_len, embedding_matrix,
                     embedding_dims=300):
    """
    Build a token encoder model for a classification task.
    Args:
        num_classes: int: number of classes for classification
        vocab_size: int: vocab_size
        max_seq_len: int: max number of features or token types considered
        embedding_matrix: np.array: matrix of [word, word_vector]
        embedding_dims: int: word vector dimensions.

    Returns: keras.models.Model
    """
    sequence_input = Input(shape=(max_seq_len,))
    x = Embedding(vocab_size,
                  embedding_dims,
                  weights=[embedding_matrix])(sequence_input)
    x = SpatialDropout1D(0.25)(x)
    x = Bidirectional(GRU(80, return_sequences=True))(x)
    x = BatchNormalization()(x)
    conc = concatenate([GlobalAveragePooling1D()(x), GlobalMaxPooling1D()(x)])
    outp = Dense(num_classes, activation="softmax")(conc)

    model = Model(inputs=sequence_input, outputs=outp)
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model


def build_token_model(max_seq_len, vocab_size, embed_dims, embed_matrix, token_encoder_model, num_classes,
                      trainable=True, output_activation='softmax'):
    """Builds a model using the given `text_model`
    Args:
        token_encoder_model: An instance of `SequenceEncoderBase` for encoding all the tokens within a document.
            This encoding is then fed into a final `Dense` layer for classification.
        trainable_embeddings: Whether or not to fine tune embeddings.
        output_activation: The output activation to use. (Default value: 'softmax')
            Use:
            - `softmax` for binary or multi-class.
            - `sigmoid` for multi-label classification.
            - `linear` for regression output.
    Returns:
        The model output tensor.
    """

    embedding_layer = Embedding(vocab_size,
                                embed_dims,
                                weights=[embed_matrix],
                                trainable=trainable)

    sequence_input = Input(shape=(max_seq_len,), dtype='int32')
    x = embedding_layer(sequence_input)
    x = token_encoder_model(x)
    x = Dense(num_classes, activation=output_activation)(x)
    return Model(sequence_input, x, name="token_encoder")


# def build_sentence_model(max_seq_len, vocab_size, max_sents, embed_dims, embed_matrix, token_encoder_model,
#                          sent_encoder_model, num_classes, trainable=True, output_activation='softmax'):
#     """Builds a model that first encodes all words within sentences using `token_encoder_model`, followed by
#     `sentence_encoder_model`.
#     Args:
#         token_encoder_model: An instance of `SequenceEncoderBase` for encoding tokens within sentences. This model
#             will be applied across all sentences to create a sentence encoding.
#         sentence_encoder_model: An instance of `SequenceEncoderBase` operating on sentence encoding generated by
#             `token_encoder_model`. This encoding is then fed into a final `Dense` layer for classification.
#         trainable_embeddings: Whether or not to fine tune embeddings.
#         output_activation: The output activation to use. (Default value: 'softmax')
#             Use:
#             - `softmax` for binary or multi-class.
#             - `sigmoid` for multi-label classification.
#             - `linear` for regression output.
#     Returns:
#         The model output tensor.
#     """
#
#     token_encoder_model = build_token_model(max_seq_len, vocab_size, embed_dims,
#                                             embed_matrix, token_encoder_model, num_classes,
#                                             trainable=trainable, output_activation='softmax')
#
#     doc_input = Input(shape=(max_sents, vocab_size), dtype='int32')
#     sent_encoding = TimeDistributed(token_encoder_model)(doc_input)
#     x = sent_encoder_model(sent_encoding)
#
#     x = Dense(num_classes, activation=output_activation)(x)
#     return Model(doc_input, x)
